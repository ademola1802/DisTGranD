# -*- coding: utf-8 -*-
"""roberta-cnn-lstm-custom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19KmtJhsfC310hVfUTQqXLohIKbNDoXws
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup
from transformers import RobertaModel, RobertaTokenizer
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns


class CustomAttention(nn.Module):
    def __init__(self, input_dim):
        super(CustomAttention, self).__init__()
        self.W = nn.Parameter(torch.randn(input_dim, 1))
        nn.init.xavier_uniform_(self.W.data)
        self.b = nn.Parameter(torch.zeros(input_dim))

    def forward(self, x):
        e = F.relu(torch.matmul(x, self.W) + self.b)
        a = torch.softmax(e, dim=1)
        output = torch.sum(x * a, dim=1)
        return output

class MultiSampleDropout(nn.Module):
    def __init__(self, dropout_prob=0.5, num_samples=5):
        super(MultiSampleDropout, self).__init__()
        self.dropout_prob = dropout_prob
        self.num_samples = num_samples

    def forward(self, x):
        dropout_samples = [F.dropout(x, p=self.dropout_prob, training=self.training) for _ in range(self.num_samples)]
        return torch.mean(torch.stack(dropout_samples, dim=0), dim=0)

class MyModel(nn.Module):
    def __init__(self, num_classes):
        super(MyModel, self).__init__()
        # Initialize RoBERTa model
        self.roberta = RobertaModel.from_pretrained('roberta-base')

        # Adjustments for the model layers if necessary
        self.lstm = nn.LSTM(input_size=768, hidden_size=512, num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)
        self.cnn_layers = nn.Sequential(
            nn.Conv1d(in_channels=768, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.MaxPool1d(kernel_size=2)
        )

        self.attention = CustomAttention(input_dim=512)
        self.dropout = MultiSampleDropout(dropout_prob=0.3, num_samples=5)
        self.fc1 = nn.Linear(1088, 256)
        self.fc2 = nn.Linear(256, num_classes)



    def forward(self, input_ids, attention_mask):
        roberta_output = self.roberta(input_ids, attention_mask=attention_mask)[0]
        lstm_output, _ = self.lstm(roberta_output)
        lstm_output_combined = lstm_output[:, :, :512] + lstm_output[:, :, 512:]

        cnn_output = self.cnn_layers(roberta_output.transpose(1, 2))
        cnn_output = F.adaptive_max_pool1d(cnn_output, 1).view(cnn_output.size(0), -1)

        attention_output = self.attention(lstm_output_combined)
        lstm_output_reduced = lstm_output_combined.mean(dim=1)

        combined = torch.cat((cnn_output, attention_output, lstm_output_reduced), dim=1)

        x = F.relu(self.fc1(combined))
        x = self.dropout(x)
        x = self.fc2(x)
        return x




# Data Preparation and Model Training
def preprocess_text(text):
    text = text.replace("URL", "")
    text = ''.join([c for c in text if c.isalnum() or c.isspace()])
    return text

class TweetDataset(Dataset):
    def __init__(self, tweets, labels, tokenizer, max_len):
        self.tweets = tweets
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.tweets)

    def __getitem__(self, item):
        tweet = str(self.tweets[item])
        label = self.labels[item]
        encoding = self.tokenizer.encode_plus(
            tweet,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'tweet_text': tweet,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')




# loading and preprocessing data
df_train = pd.read_csv("/home/aaadfg/disaster_paper/train2.csv")
df_test = pd.read_csv("/home/aaadfg/disaster_paper/test2.csv")
df_train['tweet'] = df_train['tweet'].apply(preprocess_text)
df_test['tweet'] = df_test['tweet'].apply(preprocess_text)

# Encode labels
encoded_dict = {"admiration": 0, "appreciation": 1, "business": 2, "casualty": 3,
    "climate and environmental issues": 4, "communication": 5, "damage": 6, "die": 7,
    "disaster preparedness": 8, "education": 9, "empathy": 10, "health": 11,
    "humanitarian assistance": 12, "immigration": 13, "information dissemination": 14,
    "inquiry": 15, "life": 16, "memories": 17, "news": 18, "others": 19,
    "personal matters": 20, "politics": 21, "resources": 22, "safety": 23,
    "sport": 24, "spiritual": 25, "transportation": 26, "travel": 27, "warning": 28} # Your label encoding here
df_train['event'] = df_train['event'].map(encoded_dict)
df_test['event'] = df_test['event'].map(encoded_dict)

train_dataset = TweetDataset(df_train['tweet'].to_numpy(), df_train['event'].to_numpy(), tokenizer, max_len=50)
test_dataset = TweetDataset(df_test['tweet'].to_numpy(), df_test['event'].to_numpy(), tokenizer, max_len=50)

# Adjustments for Hyperparameter Tuning and Advanced Techniques
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)
test_loader = DataLoader(test_dataset, batch_size=64, num_workers=8)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel(num_classes=len(encoded_dict)).to(device)

# Utilizing multiple GPUs if available
if torch.cuda.device_count() > 1:
    print(f"Let's use {torch.cuda.device_count()} GPUs!")
    model = nn.DataParallel(model)
model.to(device)

# AdamW optimizer is used here
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# Loss function
loss_fn = nn.CrossEntropyLoss()

# Scheduler with warm-up
num_epochs = 10
total_steps = len(train_loader) * num_epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)

# Mixed Precision Training
scaler = GradScaler()

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    all_predictions = []
    all_labels = []

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        with autocast():
            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scheduler.step()  # Update learning rate schedule
        scaler.update()
        total_loss += loss.item()

        # Move logits and labels to CPU
        outputs = outputs.detach().cpu().numpy()
        labels = labels.detach().cpu().numpy()
        predictions = np.argmax(outputs, axis=1)

        all_predictions.extend(predictions)
        all_labels.extend(labels)

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}, "
          f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")



# Evaluation with mixed precision
model.eval()
predictions, true_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        with autocast():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.cpu().tolist())
            true_labels.extend(labels.cpu().tolist())

print(classification_report(true_labels, predictions))